{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect TFRecords to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/shared/TORCS_train.tfrecords'\n",
    "\n",
    "feature = {'fast': tf.FixedLenFeature([], tf.float32),\n",
    "    'dist_RR': tf.FixedLenFeature([], tf.float32),\n",
    "    'dist_MM': tf.FixedLenFeature([], tf.float32),\n",
    "    'dist_LL': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_RR': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_MR': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_ML': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_LL': tf.FixedLenFeature([], tf.float32),\n",
    "    'dist_R': tf.FixedLenFeature([], tf.float32),\n",
    "    'dist_L': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_R': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_M': tf.FixedLenFeature([], tf.float32),\n",
    "    'toMarking_L': tf.FixedLenFeature([], tf.float32),\n",
    "    'angle': tf.FixedLenFeature([], tf.float32),\n",
    "    'image': tf.FixedLenFeature([], tf.string)}\n",
    "\n",
    "# Create a list of filenames and pass it to a queue\n",
    "filename_queue = tf.train.string_input_producer([data_path], num_epochs=100)\n",
    "# Define a reader and read the next record\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "# Decode the record read by the reader\n",
    "features = tf.parse_single_example(serialized_example, features=feature)\n",
    "# Convert the image data from string back to the numbers\n",
    "image = tf.decode_raw(features['image'], tf.uint8)\n",
    "\n",
    "# Cast label data into float32\n",
    "fast = tf.cast(features['fast'], tf.float32)\n",
    "\n",
    "# rescale all targets to [0,1], numbers were taken from source code of DeepDriving\n",
    "dist_RR = tf.cast(features['dist_RR'], tf.float32) / 95.0 + 0.12\n",
    "dist_MM = tf.cast(features['dist_MM'], tf.float32) / 95.0 + 0.12\n",
    "dist_LL = tf.cast(features['dist_LL'], tf.float32) / 95.0 + 0.12\n",
    "\n",
    "toMarking_RR = tf.cast(features['toMarking_RR'], tf.float32) / 6.8752 - 0.48181\n",
    "toMarking_MR = tf.cast(features['toMarking_MR'], tf.float32) / 6.25 + 0.02\n",
    "toMarking_ML = tf.cast(features['toMarking_ML'], tf.float32) / 6.25 + 0.98\n",
    "toMarking_LL = tf.cast(features['toMarking_LL'], tf.float32) / 6.8752 + 1.48181\n",
    "\n",
    "dist_R = tf.cast(features['dist_R'], tf.float32) / 95.0 + 0.12\n",
    "dist_L = tf.cast(features['dist_L'], tf.float32) / 95.0 + 0.12\n",
    "\n",
    "toMarking_R = tf.cast(features['toMarking_R'], tf.float32) / 5.6249 - 0.34445\n",
    "toMarking_M = tf.cast(features['toMarking_M'], tf.float32) / 6.8752 + 0.39091\n",
    "toMarking_L = tf.cast(features['toMarking_L'], tf.float32) / 5.6249 + 1.34445\n",
    "\n",
    "angle = tf.cast(features['angle'], tf.float32) / 1.1 + 0.5\n",
    "\n",
    "# Reshape image data into the original shape\n",
    "image = tf.reshape(image, [210, 280, 3])\n",
    "\n",
    "# rescale from [0..255] to [-1..1]\n",
    "image = tf.cast(image, tf.float32)\n",
    "image = (image - 128.0) / 128.0\n",
    "\n",
    "# Creates batches by randomly shuffling tensors\n",
    "#images=1\n",
    "images, fasts, dist_RRs, dist_MMs, dist_LLs, toMarking_RRs, toMarking_MRs, toMarking_MLs, toMarking_LLs, dist_Rs, dist_Ls, toMarking_Rs, toMarking_Ms, toMarking_Ls, angles = tf.train.shuffle_batch(\n",
    "                    [image, fast, dist_RR, dist_MM, dist_LL, toMarking_RR, toMarking_MR, toMarking_ML,\n",
    "                        toMarking_LL, dist_R, dist_L, toMarking_R, toMarking_M, toMarking_L, angle],\n",
    "                        batch_size=64, capacity=50000, num_threads=8, min_after_dequeue=10000, allow_smaller_final_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.concat([tf.expand_dims(fasts,1), tf.expand_dims(dist_RRs,1), tf.expand_dims(dist_MMs,1), tf.expand_dims(dist_LLs,1), \n",
    "                    tf.expand_dims(toMarking_RRs,1), tf.expand_dims(toMarking_MRs,1), tf.expand_dims(toMarking_MLs,1),\n",
    "                    tf.expand_dims(toMarking_LLs,1), tf.expand_dims(dist_Rs,1), tf.expand_dims(dist_Ls,1), tf.expand_dims(toMarking_Rs,1),\n",
    "                    tf.expand_dims(toMarking_Ms,1), tf.expand_dims(toMarking_Ls,1), tf.expand_dims(angles,1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from alexnet import alexnet_v2\n",
    "\n",
    "\n",
    "logits, endpoints = alexnet_v2(images,\n",
    "               num_classes=1000,\n",
    "               is_training=True,\n",
    "               dropout_keep_prob=0.999,\n",
    "               spatial_squeeze=True,\n",
    "               scope='alexnet_v2')\n",
    "\n",
    "y_pred = endpoints['alexnet_v2/fc8']\n",
    "loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input and output Tensor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('alexnet_v2/conv1',\n",
       "              <tf.Tensor 'alexnet_v2/conv1/Relu:0' shape=(64, 50, 68, 64) dtype=float32>),\n",
       "             ('alexnet_v2/pool1',\n",
       "              <tf.Tensor 'alexnet_v2/pool1/MaxPool:0' shape=(64, 24, 33, 64) dtype=float32>),\n",
       "             ('alexnet_v2/conv2',\n",
       "              <tf.Tensor 'alexnet_v2/conv2/Relu:0' shape=(64, 24, 33, 192) dtype=float32>),\n",
       "             ('alexnet_v2/pool2',\n",
       "              <tf.Tensor 'alexnet_v2/pool2/MaxPool:0' shape=(64, 11, 16, 192) dtype=float32>),\n",
       "             ('alexnet_v2/conv3',\n",
       "              <tf.Tensor 'alexnet_v2/conv3/Relu:0' shape=(64, 11, 16, 384) dtype=float32>),\n",
       "             ('alexnet_v2/conv4',\n",
       "              <tf.Tensor 'alexnet_v2/conv4/Relu:0' shape=(64, 11, 16, 384) dtype=float32>),\n",
       "             ('alexnet_v2/conv5',\n",
       "              <tf.Tensor 'alexnet_v2/conv5/Relu:0' shape=(64, 11, 16, 256) dtype=float32>),\n",
       "             ('alexnet_v2/pool5',\n",
       "              <tf.Tensor 'alexnet_v2/pool5/MaxPool:0' shape=(64, 5, 7, 256) dtype=float32>),\n",
       "             ('alexnet_v2/fc6',\n",
       "              <tf.Tensor 'alexnet_v2/fc6/Relu:0' shape=(64, 1, 1, 4096) dtype=float32>),\n",
       "             ('alexnet_v2/fc7',\n",
       "              <tf.Tensor 'alexnet_v2/fc7/Relu:0' shape=(64, 1, 1, 4096) dtype=float32>),\n",
       "             ('alexnet_v2/fc8',\n",
       "              <tf.Tensor 'alexnet_v2/fc8/squeezed:0' shape=(64, 14) dtype=float32>)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from mobilenet_v1 import mobilenet_v1\n",
    "\n",
    "# logits, endpoints = mobilenet_v1(images, #tf.cast(images, tf.float32),\n",
    "#                  num_classes=1000,\n",
    "#                  dropout_keep_prob=0.999,\n",
    "#                  is_training=True,\n",
    "#                  min_depth=8,\n",
    "#                  depth_multiplier=0.75,\n",
    "#                  conv_defs=None,\n",
    "#                  prediction_fn=tf.contrib.layers.softmax,\n",
    "#                  spatial_squeeze=True,\n",
    "#                  reuse=None,\n",
    "#                  scope='MobilenetV1')\n",
    "\n",
    "# y_pred = endpoints['Logits']\n",
    "# loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:model_alex/19999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/39999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/59999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/79999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/99999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/119999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/139999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_t = time()\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    loss_list = []\n",
    "    for batch_index in range(140000):\n",
    "        loss_val, _ = sess.run([loss, train_step])\n",
    "        loss_list.append((loss_val, time() - start_t))\n",
    "        if batch_index % 10 == 0:\n",
    "            np.save('loss.npy', loss_list)\n",
    "        if (batch_index+1) % 20000 == 0:\n",
    "            saver.save(sess, \"model_alex/{}.ckpt\".format(batch_index))\n",
    "    \n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "\n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
